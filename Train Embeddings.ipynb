{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings  - building out categorical embeddings from:  \n",
    "* https://www.fast.ai/2018/04/29/categorical-embeddings/\n",
    "* https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa\n",
    "* https://forums.fast.ai/t/understanding-columnarmodeldata-from-data-frame-from-rossman/8140\n",
    "* https://yashuseth.blog/2018/07/22/pytorch-neural-network-for-tabular-data-with-categorical-embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader as torch_dl\n",
    "from torch.utils.data import Dataset\n",
    "from torch import  nn\n",
    "from torch import optim\n",
    "from torch.nn.init import *\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, scale\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielamaranto/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by the scale function.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#directory='c:/users/da1933/desktop/cds/ids_project/'\n",
    "directory='~/desktop/idsproject/'\n",
    "\n",
    "df1=pd.read_csv(directory+'wine/winemag-data-130k-v2.csv',index_col=0)\n",
    "df2=pd.read_csv(directory+'wine/winemag-data_first150k.csv',index_col=0)\n",
    "df=pd.concat([df1.drop(['taster_name','taster_twitter_handle','title'],axis=1),df2]).reset_index(drop=True)\n",
    "\n",
    "df['over90']=df['points'].apply(lambda x: 1 if x>=90 else 0)\n",
    "df['textlen']=df['description'].apply(lambda x: len(x))\n",
    "df['textlen']=scale(df[['textlen']])\n",
    "df['price']=df['price'].fillna(np.mean(df['price']))\n",
    "df['price']=scale(df[['price']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country 50 nan:  68\n",
      "variety 756 nan:  1\n",
      "continent 6 nan:  69\n"
     ]
    }
   ],
   "source": [
    "continents=pickle.load(open('continents.sav','rb'))\n",
    "df['continent']=df['country'].map(continents)\n",
    "\n",
    "categorical_features=['country',\n",
    "                      'variety',\n",
    "                      'continent']\n",
    "\n",
    "for i in categorical_features:\n",
    "    print(i,df[i].nunique(),'nan: ',sum(df[i].isna()))\n",
    "    df[i]=df[i].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.set_index(np.random.permutation(df.index.values)).sort_index()\n",
    "\n",
    "# Determine split index:\n",
    "split_index=int(np.round(df.shape[0]*.8))\n",
    "\n",
    "# Assign training and testing datasets based on split index\n",
    "train_df=df.iloc[:split_index,:]\n",
    "test_df=df.iloc[split_index:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#begin with regression problem\n",
    "#output_feature='points'\n",
    "#data=df[categorical_features+['textlen','points']]\n",
    "\n",
    "output_feature='over90'\n",
    "#data=train_df[categorical_features+['textlen','over90','price']]\n",
    "#testdata=test_df[categorical_features+['textlen','over90','price']]\n",
    "testdata=test_df[categorical_features+['over90','price']]\n",
    "data=train_df[categorical_features+['over90','price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, data, cat_cols=None, output_col=None):\n",
    "        \"\"\"\n",
    "        Characterizes a Dataset for PyTorch\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        data: pandas data frame\n",
    "          The data frame object for the input data. It must\n",
    "          contain all the continuous, categorical and the\n",
    "          output columns to be used.\n",
    "\n",
    "        cat_cols: List of strings\n",
    "          The names of the categorical columns in the data.\n",
    "          These columns will be passed through the embedding\n",
    "          layers in the model. These columns must be\n",
    "          label encoded beforehand. \n",
    "\n",
    "        output_col: string\n",
    "          The name of the output variable column in the data\n",
    "          provided.\n",
    "        \"\"\"\n",
    "\n",
    "        self.n = data.shape[0]\n",
    "\n",
    "        if output_col:\n",
    "            self.y = data[output_col].astype(np.float32).values.reshape(-1, 1)\n",
    "        else:\n",
    "            self.y =  np.zeros((self.n, 1))\n",
    "\n",
    "        self.cat_cols = cat_cols if cat_cols else []\n",
    "        self.cont_cols = [col for col in data.columns\n",
    "                          if col not in self.cat_cols + [output_col]]\n",
    "\n",
    "        if self.cont_cols:\n",
    "            self.cont_X = data[self.cont_cols].astype(np.float32).values\n",
    "        else:\n",
    "            self.cont_X = np.zeros((self.n, 1))\n",
    "\n",
    "        if self.cat_cols:\n",
    "            self.cat_X = data[cat_cols].astype(np.int64).values\n",
    "        else:\n",
    "            self.cat_X =  np.zeros((self.n, 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Denotes the total number of samples.\n",
    "        \"\"\"\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generates one sample of data.\n",
    "        \"\"\"\n",
    "        return [self.y[idx], self.cont_X[idx], self.cat_X[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The nn.Module class is the base class for all neural networks in PyTorch. \n",
    "#Our model, FeedForwardNN will subclass the nn.Module class. \n",
    "#In the __init__ method of our class, we will initialize the various layers \n",
    "#that will be used in the model and the forward method would define the \n",
    "#various computations performed in the network.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dims, no_of_cont, lin_layer_sizes,\n",
    "               output_size, emb_dropout, lin_layer_dropouts):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        emb_dims: List of two element tuples\n",
    "          This list will contain a two element tuple for each\n",
    "          categorical feature. The first element of a tuple will\n",
    "          denote the number of unique values of the categorical\n",
    "          feature. The second element will denote the embedding\n",
    "          dimension to be used for that feature.\n",
    "\n",
    "        no_of_cont: Integer\n",
    "          The number of continuous features in the data.\n",
    "\n",
    "        lin_layer_sizes: List of integers.\n",
    "          The size of each linear layer. The length will be equal\n",
    "          to the total number\n",
    "          of linear layers in the network.\n",
    "\n",
    "        output_size: Integer\n",
    "          The size of the final output.\n",
    "\n",
    "        emb_dropout: Float\n",
    "          The dropout to be used after the embedding layers.\n",
    "\n",
    "        lin_layer_dropouts: List of floats\n",
    "          The dropouts to be used after each linear layer.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layers\n",
    "        self.emb_layers = nn.ModuleList([nn.Embedding(x, y)\n",
    "                                         for x, y in emb_dims])\n",
    "\n",
    "        no_of_embs = sum([y for x, y in emb_dims])\n",
    "        self.no_of_embs = no_of_embs\n",
    "        self.no_of_cont = no_of_cont\n",
    "\n",
    "        # Linear Layers\n",
    "        first_lin_layer = nn.Linear(self.no_of_embs + self.no_of_cont,\n",
    "                                    lin_layer_sizes[0])\n",
    "\n",
    "        self.lin_layers =\\\n",
    "         nn.ModuleList([first_lin_layer] +\\\n",
    "              [nn.Linear(lin_layer_sizes[i], lin_layer_sizes[i + 1])\n",
    "               for i in range(len(lin_layer_sizes) - 1)])\n",
    "\n",
    "        for lin_layer in self.lin_layers:\n",
    "            nn.init.kaiming_normal(lin_layer.weight.data)\n",
    "\n",
    "        # Output Layer\n",
    "        self.output_layer = nn.Linear(lin_layer_sizes[-1],\n",
    "                                      output_size)\n",
    "        nn.init.kaiming_normal(self.output_layer.weight.data)\n",
    "\n",
    "        # Batch Norm Layers\n",
    "        self.first_bn_layer = nn.BatchNorm1d(self.no_of_cont)\n",
    "        self.bn_layers = nn.ModuleList([nn.BatchNorm1d(size)\n",
    "                                        for size in lin_layer_sizes])\n",
    "\n",
    "        # Dropout Layers\n",
    "        self.emb_dropout_layer = nn.Dropout(emb_dropout)\n",
    "        self.droput_layers = nn.ModuleList([nn.Dropout(size)\n",
    "                                      for size in lin_layer_dropouts])\n",
    "\n",
    "    def forward(self, cont_data, cat_data):\n",
    "\n",
    "        if self.no_of_embs != 0:\n",
    "            x = [emb_layer(cat_data[:, i])\n",
    "               for i,emb_layer in enumerate(self.emb_layers)]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.emb_dropout_layer(x)\n",
    "\n",
    "        if self.no_of_cont != 0:\n",
    "            normalized_cont_data = self.first_bn_layer(cont_data)\n",
    "\n",
    "            if self.no_of_embs != 0:\n",
    "                x = torch.cat([x, normalized_cont_data], 1) \n",
    "            else:\n",
    "                x = normalized_cont_data\n",
    "\n",
    "        for lin_layer, dropout_layer, bn_layer in\\\n",
    "            zip(self.lin_layers, self.droput_layers, self.bn_layers):\n",
    "            x = F.relu(lin_layer(x))\n",
    "            x = bn_layer(x)\n",
    "            x = dropout_layer(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielamaranto/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# After creating the network architecture we have to run the training loop. \n",
    "# For the purpose of demonstration, I am using the dataset from the Kaggle \n",
    "# competition – House Prices: Advanced Regression Techniques.\n",
    "\n",
    "# We need to instantiate an object of the TabularData class we created earlier.\n",
    "# But before that, we need to label encode the categorical features. \n",
    "# For this, we will be using sklearn.preprocessing.LabelEncoder.\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoders = {}\n",
    "for cat_col in categorical_features:\n",
    "        label_encoders[cat_col] = LabelEncoder()\n",
    "        data[cat_col] = label_encoders[cat_col].fit_transform(data[cat_col])\n",
    "\n",
    "#Let’s instantiate an object of the TabularDataset class.\n",
    "dataset = TabularDataset(data=data, cat_cols=categorical_features,\n",
    "                             output_col=output_feature)\n",
    "testdataset = TabularDataset(data=data, cat_cols=categorical_features,\n",
    "                             output_col=output_feature)\n",
    "\n",
    "# In order to run the training loop, we need to create a torch.util.data.Dataloader\n",
    "# object. It serves the following purpose –\n",
    "### creates batches from the dataset\n",
    "### shuffles the data\n",
    "### loads the data in parallel\n",
    "\n",
    "batchsize = 128\n",
    "dataloader = DataLoader(dataset, batchsize, shuffle=True)\n",
    "\n",
    "# Now that we have created the basic data structure to run the training loop,\n",
    "# we need to instantiate a model object of the FeedForwadNN class created earlier.\n",
    "# This class requires a list of tuples, where each tuple represents a pair of total \n",
    "# and the embedding dimension of a categorical variable.\n",
    "\n",
    "cat_dims = [int(data[col].nunique()) for col in categorical_features]\n",
    "emb_dims = [(x, min(50, (x + 1) // 2)) for x in cat_dims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=[i**2 for i in range(7,20,3)]\n",
    "l2=[i**2 for i in range(7,20,3)]\n",
    "\n",
    "do=[.01,.05,.1]\n",
    "\n",
    "# The number of continuous features used is 1. The hidden layer dimension is 50 and 100\n",
    "# for the first and second layers respectively. The embedding dropout used is 0.04. \n",
    "# The hidden layer dropouts are 0.001 and 0.01.\n",
    "acc=[]\n",
    "\n",
    "for lin1 in l1:\n",
    "    for lin2 in l2:\n",
    "        for d1 in do:\n",
    "            for d2 in do:\n",
    "                model = FeedForwardNN(emb_dims, no_of_cont=2, lin_layer_sizes=[lin1, lin2],\\\n",
    "                                      output_size=1, emb_dropout=d1,\\\n",
    "                                      lin_layer_dropouts=[d1,d2])\n",
    "\n",
    "                # Finally, let’s run the training loop –\n",
    "\n",
    "                no_of_epochs = 20\n",
    "                criterion = nn.MSELoss()    # Regression\n",
    "                #criterion = nn.HingeEmbeddingLoss() #??????\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "                    \n",
    "                for epoch in range(no_of_epochs):\n",
    "                    for y, cont_x, cat_x in dataloader:\n",
    "\n",
    "                        cat_x = cat_x\n",
    "                        cont_x = cont_x\n",
    "                        y  = y\n",
    "\n",
    "                        # Forward Pass\n",
    "                        preds = model(Variable(cont_x), cat_x)\n",
    "                        loss = criterion(preds, Variable(y))\n",
    "\n",
    "                        # Backward Pass and Optimization\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    if epoch%5==0:\n",
    "                        print(epoch,loss)\n",
    "\n",
    "                testcat=testdataset.cat_X[:100000,:]\n",
    "                testcont=testdataset.cont_X[:100000,:]\n",
    "                testy=testdataset.y[:100000]\n",
    "\n",
    "                model.eval()\n",
    "\n",
    "                testpreds=model(Variable(torch.from_numpy(testcont)),Variable(torch.from_numpy(testcat)))\n",
    "\n",
    "                accuracy=sum(np.round(testpreds.data.numpy().flatten('C'))==testy.flatten('C'))/len(testy)\n",
    "\n",
    "                print('Accuracy:',accuracy)\n",
    "\n",
    "                acc.append((lin1,lin2,d1,d2,accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169, 100, 0.1, 0.01, 0.76611)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc[np.argmax([i[-1] for i in acc])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Variable containing:\n",
      " 0.1727\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "1 Variable containing:\n",
      " 0.2118\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "2 Variable containing:\n",
      " 0.1650\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "3 Variable containing:\n",
      " 0.1879\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "4 Variable containing:\n",
      " 0.1766\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Accuracy: 0.7630172525042164\n"
     ]
    }
   ],
   "source": [
    "model = FeedForwardNN(emb_dims, no_of_cont=1, lin_layer_sizes=[169, 100],\\\n",
    "                      output_size=1, emb_dropout=.1,\\\n",
    "                      lin_layer_dropouts=[.1,.01])\n",
    "\n",
    "# Finally, let’s run the training loop –\n",
    "\n",
    "no_of_epochs = 5\n",
    "criterion = nn.MSELoss()    # Regression\n",
    "#criterion = nn.HingeEmbeddingLoss() #??????\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(no_of_epochs):\n",
    "    for y, cont_x, cat_x in dataloader:\n",
    "\n",
    "        cat_x = cat_x\n",
    "        cont_x = cont_x\n",
    "        y  = y\n",
    "\n",
    "        # Forward Pass\n",
    "        preds = model(Variable(cont_x), cat_x)\n",
    "        loss = criterion(preds, Variable(y))\n",
    "\n",
    "        # Backward Pass and Optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(epoch,loss)\n",
    "\n",
    "testcat=testdataset.cat_X\n",
    "testcont=testdataset.cont_X\n",
    "testy=testdataset.y\n",
    "\n",
    "model.eval()\n",
    "\n",
    "testpreds=model(Variable(torch.from_numpy(testcont)),Variable(torch.from_numpy(testcat)))\n",
    "\n",
    "accuracy=sum(np.round(testpreds.data.numpy().flatten('C'))==testy.flatten('C'))/len(testy)\n",
    "\n",
    "print('Accuracy:',accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
